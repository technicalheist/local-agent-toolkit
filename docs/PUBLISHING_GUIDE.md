# üöÄ Local Agent Toolkit - PyPI Publishing Guide

Your project is now ready for PyPI publishing! Here's your complete setup summary:

## ‚úÖ What's Been Set Up

### 1. **Package Structure** 
- ‚úÖ Modern `pyproject.toml` configuration
- ‚úÖ Traditional `setup.py` for compatibility  
- ‚úÖ `MANIFEST.in` for file inclusion
- ‚úÖ `LICENSE` file (MIT License)
- ‚úÖ Comprehensive `README.md` for PyPI
- ‚úÖ `.env.example` for configuration template

### 2. **Command Line Interface**
- ‚úÖ Entry point: `local-agent` command
- ‚úÖ Maps to `app:main` function
- ‚úÖ Supports interactive and single-question modes

### 3. **Python Library Interface**
- ‚úÖ Main function: `run_agent_with_question()`
- ‚úÖ Importable agents: `OllamaAgent`, `OpenAIAgent`
- ‚úÖ Tool functions available for direct use

### 4. **Environment Management**
Your project supports three approaches for managing `.env` files:

#### **Option 1: Global Project .env**
```bash
# Copy template and edit
cp .env.example .env
# Edit with your settings
nano .env
```

#### **Option 2: Per-Project Configuration**
```python
# In your Python code
import os
from dotenv import load_dotenv

# Load from specific path
load_dotenv('/path/to/your/project/.env')

# Or set programmatically  
os.environ['CURRENT_AGENT'] = 'OPENAI'
os.environ['OPENAI_API_KEY'] = 'your_key_here'

from local_agent_toolkit import run_agent_with_question
result, messages = run_agent_with_question("Your question")
```

#### **Option 3: Environment Variables**
```bash
# Set system environment variables
export CURRENT_AGENT=OLLAMA
export OLLAMA_MODEL=llama3.1
export OLLAMA_BASE_URL=http://localhost:11434

# Or in Docker/container environments
ENV CURRENT_AGENT=OLLAMA
ENV OLLAMA_BASE_URL=http://ollama-server:11434
```

## üöÄ Publishing to PyPI

### Prerequisites
1. **PyPI Account**: Register at [pypi.org](https://pypi.org/account/register/)
2. **API Token**: Generate at [pypi.org/manage/account/token/](https://pypi.org/manage/account/token/)
3. **Test PyPI Account** (recommended): Register at [test.pypi.org](https://test.pypi.org/account/register/)

### Method 1: Using the Publish Script (Recommended)
```bash
# Make sure you're in your virtual environment
source .venv/bin/activate

# Run the interactive publish script
./publish.sh
```

### Method 2: Manual Publishing
```bash
# Activate virtual environment
source .venv/bin/activate

# Clean previous builds
rm -rf dist/ build/ *.egg-info/

# Build the package
python -m build

# Check the package
twine check dist/*

# Upload to Test PyPI (recommended first)
twine upload --repository testpypi dist/*

# Upload to Production PyPI
twine upload dist/*
```

### Method 3: Using GitHub Actions
The project includes automated publishing workflow. Just:
1. Push a tag: `git tag v0.1.0 && git push origin v0.1.0`
2. Create a GitHub release
3. The workflow will automatically publish to PyPI

## üì¶ Usage After Publishing

### Command Line Tool
```bash
# Install from PyPI
pip install local-agent-toolkit

# Use as command line tool
local-agent "What files are in the current directory?"

# Interactive mode
local-agent

# With options
local-agent "Analyze code" --no-save --no-stream
```

### Python Library
```python
# Install from PyPI
# pip install local-agent-toolkit

# Basic usage
from local_agent_toolkit import run_agent_with_question

result, messages = run_agent_with_question(
    "Create a Python script that lists all files"
)
print(result)

# Advanced usage
result, messages = run_agent_with_question(
    question="What's the project structure?",
    save_messages=True,
    messages_file="analysis.json", 
    stream=False
)

# Using specific agents
from local_agent_toolkit import OllamaAgent, OpenAIAgent
```

## üîß Configuration Examples

### For Ollama Users
```bash
# .env file
CURRENT_AGENT=OLLAMA
OLLAMA_MODEL=llama3.1
OLLAMA_BASE_URL=http://localhost:11434
```

### For OpenAI Users  
```bash
# .env file
CURRENT_AGENT=OPENAI
OPENAI_API_KEY=sk-your-key-here
OPENAI_MODEL=gpt-4
```

### For Docker Deployment
```dockerfile
FROM python:3.11-slim

WORKDIR /app
RUN pip install local-agent-toolkit

# Set environment variables
ENV CURRENT_AGENT=OLLAMA
ENV OLLAMA_BASE_URL=http://ollama-server:11434  
ENV OLLAMA_MODEL=llama3.1

CMD ["local-agent"]
```

## üéØ Next Steps

1. **Test Locally**: Make sure everything works in your virtual environment
2. **Test PyPI**: Upload to Test PyPI first: `twine upload --repository testpypi dist/*`
3. **Test Installation**: `pip install --index-url https://test.pypi.org/simple/ local-agent-toolkit`
4. **Production PyPI**: Upload to production: `twine upload dist/*`
5. **GitHub Release**: Create a release on GitHub for version tracking

## üîó Important Links

- **PyPI Project**: https://pypi.org/project/local-agent-toolkit/ (after publishing)
- **Test PyPI**: https://test.pypi.org/project/local-agent-toolkit/ (for testing)
- **GitHub Repository**: https://github.com/technicalheist/local-agent-toolkit
- **Website**: https://technicalheist.com
- **Contact**: contact@technicalheist.com

## üõ†Ô∏è Troubleshooting

### Common Issues:
1. **Import Errors**: Make sure your `.env` file is in the right location
2. **Missing Dependencies**: Ensure Ollama is running (for Ollama agent) or API key is set (for OpenAI agent)
3. **Permission Errors**: Check file permissions for the working directory

### Environment Priority:
1. Programmatically set environment variables (highest priority)
2. `.env` file in current working directory
3. `.env` file in package installation directory
4. System environment variables (lowest priority)

---

üéâ **Your Local Agent Toolkit is ready for the world!** üéâ
